{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introductions\n",
    "\n",
    "## Author\n",
    "\n",
    "John Cinquegrana   \n",
    "<I need a new professional email that isn't my school one>\n",
    "\n",
    "## Preamble\n",
    "\n",
    "My first adventure into actually writing a reinforcement learning application. I've studied them in a college setting before, but haven't written one. This is used only for getting used to the gym library, don't expect any good models or results out of it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Cart Pole Problem\n",
    "\n",
    "The cart pole problem is a very simple one, perfect for getting used to everything. You have to balance a pole on top of a cart that is restricted to a single dimension of movement. It will make much more sense when you see it live.\n",
    "\n",
    "Here is an example of using keras with an OpenAI Gym environment: https://keras.io/examples/rl/actor_critic_cartpole/  \n",
    "Here is an example of using the Cart Pole problem in Gym: https://gym.openai.com/docs/#environments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Setup\n",
    "## Environment Setup\n",
    "Before we set up the agent we should know the details about our environment, and the input it will be receiving.\n",
    "### Initialization\n",
    "Import and load in the CartPole environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "source": [
    "### Outputs\n",
    "\n",
    "See what the actual values we'll be sampling from the environment look like"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Discrete(2)\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 4\n",
    "num_actions = 2"
   ]
  },
  {
   "source": [
    "Take note of all the most important data from the printouts that we will be using later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Agent Setup\n",
    "### Imports\n",
    "\n",
    "Verify that every single tf import works correctly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "source": [
    "### Model\n",
    "\n",
    "Create a simple model we will use for Q-learning later on"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nTotal params: 4,545\nTrainable params: 4,545\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "input_layer = Input( shape=(input_shape,) )\n",
    "h1 =  Dense( 64, activation=\"relu\" )(input_layer)\n",
    "h2 =  Dense( 64, activation=\"relu\" )(h1)\n",
    "final = Dense( 1, \"relu\" )(h2)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=final)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## General Idea\n",
    "We are using a Q-network over a RL problem that acts in a frame-by-frame environment over different episodes. Compared to normal machine learning, episodes are comparable to epochs, and frames to a single ovservation.\n",
    "\n",
    "To explore more options for our model I will be using the Epsilon-Greedy Action Selection algorithm. This means that in the beginning, our model will tend to select random actions to perform. This gives it a *broader idea of the possible state space*. As the model continues through training more often it will use itself, instead of luck, to pick an action.\n",
    "\n",
    "When the model does pick an action, it will pick the action it believes leads to the best state. It does not perform any searches, min-maxing, or the like. In this way it performs in a very short sighted way. This kind of reaction is fine, if not preferrable, in the CartPole problem.\n",
    "\n",
    "The algorithm is well described in [this article](https://www.baeldung.com/cs/epsilon-greedy-q-learning) on the Baeldung CS website. Though I am using a network rather than a table, because I think that's more fun.\n",
    "\n",
    "## Setting up the problem\n",
    "\n",
    "We will need several variables before we jump into the event loop. All of those are defined below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We will be running 1 episodes. Each can have up to 1000 frames. The initial chance to take a random action is 0.8.\n"
     ]
    }
   ],
   "source": [
    "#   A basic optimizer and loss function\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_function = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "# These variables will be used for Temporal Difference training\n",
    "previous_reward = 0.0\n",
    "current_reward = 0.0\n",
    "discount = 0.1\n",
    "\n",
    "\n",
    "# These variables simply keep track of our progres through training\n",
    "episode_count = 0\n",
    "frame_count = 0  # The CartPole problem has a maximum of 1000 frames before it stops itself\n",
    "last_episode = 1\n",
    "\n",
    "# These variables will be used to determine how often to give a random action\n",
    "epsilon = 0.8\n",
    "\n",
    "# These are purely informational variables that won't be used for actual training\n",
    "random_frame_count = 0\n",
    "calculated_frame_count = 0\n",
    "frames_per_episode = []\n",
    "\n",
    "# Print out basic information\n",
    "print( \"We will be running {} episodes. Each can have up to 1000 frames. The initial chance to take a random action is {}.\".format(\n",
    "    last_episode, epsilon\n",
    "))"
   ]
  },
  {
   "source": [
    "## Main Training Loop\n",
    "\n",
    "We create the environment and step into the main loop. Right now it simply samples random actions from the environment over and over again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error occured, closing environment manually.\nType of exception <class 'TypeError'>\nArguments of exception ('randint() takes at least 1 positional argument (0 given)',)\nrandint() takes at least 1 positional argument (0 given)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1') # Most current date CartPole environment in June 2021\n",
    "\n",
    "try:\n",
    "    for episode_count in range( last_episode+1 ):\n",
    "        observation = env.reset() # Reset our environment for a new episode and get the initial state\n",
    "        done = False # We are no longer done\n",
    "        frame_count = 0 # Restart the number of frames\n",
    "        while not done: # continue until the environment decides they are done\n",
    "            env.render()\n",
    "            frame_count += 1\n",
    "            # Decide if we will take a random action or if we will calculate one ourselves\n",
    "            chance = np.random.randint()\n",
    "\n",
    "            # Run a random action\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, _ = env.step( action )\n",
    "        # Actions to take at the end of each episode\n",
    "        # Print out statistics about the episode\n",
    "        print( \"Episode was finished after {} frames.\".format(\n",
    "            frame_count\n",
    "        ))\n",
    "except Exception as inst:\n",
    "    print(\"Error occured, closing environment manually.\")\n",
    "    env.close()\n",
    "    print( \"Type of exception {}\".format(type(inst)) )\n",
    "    print( \"Arguments of exception {}\".format(inst.args) )\n",
    "    print( inst )\n",
    "\n",
    "# We run the following after every single episode has been ran\n",
    "env.close()"
   ]
  },
  {
   "source": [
    "# Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}